{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/boussouralisa/IA-JALON4/blob/main/IA_projet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "1iDTnymMYqOd",
        "outputId": "dfd66593-03f2-4cce-a131-1810b2827e25"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "250/250 [==============================] - 329s 1s/step - loss: 0.7270 - accuracy: 0.8566 - val_loss: 0.5824 - val_accuracy: 0.8722\n",
            "Epoch 2/5\n",
            "250/250 [==============================] - 312s 1s/step - loss: 0.4741 - accuracy: 0.8765 - val_loss: 0.4201 - val_accuracy: 0.8815\n",
            "Epoch 3/5\n",
            "250/250 [==============================] - 303s 1s/step - loss: 0.4287 - accuracy: 0.8810 - val_loss: 0.4341 - val_accuracy: 0.8817\n",
            "Epoch 4/5\n",
            "250/250 [==============================] - 299s 1s/step - loss: 0.4166 - accuracy: 0.8838 - val_loss: 0.3976 - val_accuracy: 0.8894\n",
            "Epoch 5/5\n",
            "250/250 [==============================] - 308s 1s/step - loss: 0.4053 - accuracy: 0.8864 - val_loss: 0.3828 - val_accuracy: 0.8935\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "Texte prédit: wad\n"
          ]
        }
      ],
      "source": [
        "# Importer les bibliothèques nécessaires\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import xml.etree.ElementTree as ET\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense, Reshape, TimeDistributed, Bidirectional, Conv2D, MaxPooling2D, Flatten, Dropout, BatchNormalization\n",
        "from tensorflow.keras.models import Model\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Définir les chemins d'accès\n",
        "word_images_path = \"/content/data/words\"\n",
        "xml_path = \"/content/data/xml\"\n",
        "\n",
        "# Fonction pour charger les images et les étiquettes\n",
        "def load_images_and_labels(word_images_path, xml_path, image_size=(128, 32), limit=None):\n",
        "    images = []\n",
        "    labels = []\n",
        "    count = 0\n",
        "    for root_dir, dirs, files in os.walk(word_images_path):\n",
        "        for file in files:\n",
        "            if file.endswith(\".png\"):\n",
        "                image_path = os.path.join(root_dir, file)\n",
        "                image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
        "                if image is not None:\n",
        "                    resized_image = cv2.resize(image, image_size)\n",
        "                    resized_image = np.transpose(resized_image)  # Transposer l'image pour correspondre aux dimensions attendues\n",
        "                    images.append(resized_image)\n",
        "\n",
        "                    xml_file = file.split('-')[0] + \"-\" + file.split('-')[1] + \".xml\"\n",
        "                    xml_file_path = os.path.join(xml_path, xml_file)\n",
        "\n",
        "                    label_found = False\n",
        "                    if os.path.exists(xml_file_path):\n",
        "                        tree = ET.parse(xml_file_path)\n",
        "                        root = tree.getroot()\n",
        "                        for elem in root.iter('word'):\n",
        "                            if elem.attrib['id'] == file.split('.')[0]:\n",
        "                                labels.append(elem.attrib['text'])\n",
        "                                label_found = True\n",
        "                                break\n",
        "                    if not label_found:\n",
        "                        labels.append(\"\")\n",
        "                    count += 1\n",
        "                    if limit and count >= limit:\n",
        "                        return images, labels\n",
        "    return images, labels\n",
        "\n",
        "# Limiter le nombre d'images chargées pour éviter la surcharge de RAM\n",
        "images, labels = load_images_and_labels(word_images_path, xml_path, limit=10000)\n",
        "\n",
        "# Supprimer les paires (image, label) où le label est vide\n",
        "filtered_images = []\n",
        "filtered_labels = []\n",
        "for img, lbl in zip(images, labels):\n",
        "    if lbl:\n",
        "        filtered_images.append(img)\n",
        "        filtered_labels.append(lbl)\n",
        "\n",
        "# Convertir les listes en tableaux NumPy\n",
        "images = np.array(filtered_images)\n",
        "labels = np.array(filtered_labels)\n",
        "\n",
        "# Normaliser les images\n",
        "images = images / 255.0\n",
        "images = np.expand_dims(images, axis=-1)  # Ajouter une dimension pour les canaux\n",
        "\n",
        "# Encoder les labels\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(char_level=True)\n",
        "tokenizer.fit_on_texts(labels)\n",
        "sequences = tokenizer.texts_to_sequences(labels)\n",
        "max_label_length = max([len(seq) for seq in sequences])\n",
        "padded_sequences = tf.keras.preprocessing.sequence.pad_sequences(sequences, maxlen=max_label_length, padding='post')\n",
        "\n",
        "# Définir la longueur fixe pour les séquences de sortie du modèle\n",
        "output_sequence_length = 32  # Par exemple\n",
        "\n",
        "# Ajuster les séquences de labels à la longueur de sortie du modèle\n",
        "padded_sequences = tf.keras.preprocessing.sequence.pad_sequences(sequences, maxlen=output_sequence_length, padding='post')\n",
        "\n",
        "# Fractionner les données en ensembles d'entraînement et de validation\n",
        "X_train, X_val, y_train, y_val = train_test_split(images, padded_sequences, test_size=0.2, random_state=42)\n",
        "\n",
        "# Utiliser tf.data.Dataset pour la génération par lot\n",
        "def augment_image(image, label):\n",
        "    image = tf.image.random_flip_left_right(image)\n",
        "    image = tf.image.random_brightness(image, max_delta=0.1)\n",
        "    image = tf.image.random_contrast(image, lower=0.9, upper=1.1)\n",
        "    return image, label\n",
        "\n",
        "def create_dataset(X, y, batch_size=32, augment=False):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((X, y))\n",
        "    if augment:\n",
        "        dataset = dataset.map(augment_image, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "    dataset = dataset.shuffle(buffer_size=1000).batch(batch_size).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        "    return dataset\n",
        "\n",
        "# Augmenter la taille du lot\n",
        "batch_size = 32\n",
        "\n",
        "train_dataset = create_dataset(X_train, y_train, batch_size, augment=True)\n",
        "val_dataset = create_dataset(X_val, y_val, batch_size)\n",
        "\n",
        "# Définir le modèle LSTM avec des couches de régularisation\n",
        "input_data = Input(name='inputs', shape=(128, 32, 1))\n",
        "conv_1 = Conv2D(32, (3,3), activation='relu', padding='same')(input_data)\n",
        "pool_1 = MaxPooling2D(pool_size=(2, 2))(conv_1)\n",
        "conv_2 = Conv2D(64, (3,3), activation='relu', padding='same')(pool_1)\n",
        "pool_2 = MaxPooling2D(pool_size=(2, 2))(conv_2)\n",
        "conv_3 = Conv2D(128, (3,3), activation='relu', padding='same')(pool_2)\n",
        "pool_3 = MaxPooling2D(pool_size=(2, 2))(conv_3)\n",
        "flattened = Flatten()(pool_3)\n",
        "dense_flat = Dense(32*128, activation='relu')(flattened)\n",
        "dropout_1 = Dropout(0.5)(dense_flat)\n",
        "reshaped = Reshape((32, 128))(dropout_1)  # Adapter la forme pour LSTM\n",
        "dense_1 = Dense(64, activation='relu')(reshaped)\n",
        "dropout_2 = Dropout(0.5)(dense_1)\n",
        "lstm_1 = Bidirectional(LSTM(128, return_sequences=True))(dropout_2)\n",
        "lstm_2 = Bidirectional(LSTM(128, return_sequences=True))(lstm_1)\n",
        "batch_norm = BatchNormalization()(lstm_2)\n",
        "dense_2 = TimeDistributed(Dense(len(tokenizer.word_index) + 1, activation='softmax'))(batch_norm)\n",
        "\n",
        "model = Model(inputs=input_data, outputs=dense_2)\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Adapter la sortie du modèle pour qu'elle corresponde à la taille maximale des séquences des étiquettes\n",
        "y_train = np.expand_dims(y_train, axis=-1)\n",
        "y_val = np.expand_dims(y_val, axis=-1)\n",
        "\n",
        "# Entraîner le modèle avec 5 époques\n",
        "history = model.fit(train_dataset, epochs=5, validation_data=val_dataset)\n",
        "\n",
        "# Faire des prédictions sur de nouvelles images\n",
        "def predict_image(image_path, model, tokenizer):\n",
        "    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
        "    image = cv2.resize(image, (128, 32))\n",
        "    image = np.transpose(image)  # Transposer l'image pour correspondre aux dimensions attendues\n",
        "    image = image / 255.0\n",
        "    image = np.expand_dims(image, axis=-1)\n",
        "    image = np.expand_dims(image, axis=0)\n",
        "    prediction = model.predict(image)\n",
        "    predicted_indices = np.argmax(prediction, axis=-1)[0]\n",
        "    predicted_label = ''.join([tokenizer.index_word.get(idx, '') for idx in predicted_indices if idx != 0])\n",
        "    return predicted_label\n",
        "\n",
        "# Afficher les résultats de la prédiction\n",
        "test_image_path = os.path.join(word_images_path, \"a01/a01-000u/a01-000u-01-01.png\")\n",
        "predicted_label = predict_image(test_image_path, model, tokenizer)\n",
        "print(\"Texte prédit:\", predicted_label)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EblUOHdnbDSB"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1lDospOOY326jVZOm2DqBo3vMDlYpR_4j",
      "authorship_tag": "ABX9TyOsNL+L+twmGyI8S5iq4G1h",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}